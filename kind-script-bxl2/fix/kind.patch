diff --git a/Dockerfile b/Dockerfile
index eef6b90..5e74394 100644
--- a/Dockerfile
+++ b/Dockerfile
@@ -22,6 +22,10 @@ RUN --mount=type=cache,target=/root/.cache/uv \
 
 # Final image
 FROM python:3.11-slim
+
+RUN apt-get update && apt-get install -y curl \
+    && rm -rf /var/lib/apt/lists/*
+
 RUN adduser python
 USER python
 
diff --git a/config/flask.env b/config/flask.env
index e41005b..50b3780 100644
--- a/config/flask.env
+++ b/config/flask.env
@@ -4,6 +4,7 @@ DB_HOST='postgres'
 DB_NAME='smo'
 # Filename of the Karmada API server config that must be located inside the ~/.kube directory
 KARMADA_KUBECONFIG='karmada-apiserver.config'
+KARMADA_CONTEXT='karmada-apiserver'
 # Base URL of the NVFCL API
 NFVCL_BASE_URL='http://192.168.12.64:5002'
 # Flag to set if container/artifact registry doesn't use HTTPS
diff --git a/docker-compose.yml b/docker-compose.yml
index 43f8ca1..74ecb24 100644
--- a/docker-compose.yml
+++ b/docker-compose.yml
@@ -1,35 +1,46 @@
 services:
-  smo:
-    restart: always
-    container_name: smo
-    build: .
-    env_file:
-    - config/flask.env
-    volumes:
-      - type: bind
-        source: ~/.kube/
-        target: /home/python/.kube
-        read_only: true
-      - type: bind
-        source: ~/.docker/
-        target: /home/python/.docker
-        read_only: true
-    depends_on:
-      postgres:
-        condition: service_healthy
-    ports:
-      - 8000:8000
-    extra_hosts:
-    - "host.docker.internal:host-gateway"
-  postgres:
-    restart: always
-    container_name: postgres
-    image: postgres:16.2
-    env_file:
-    - config/postgres.env
-    healthcheck:
-      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB} -t 1"]
-      interval: 3s
-      timeout: 3s
-      retries: 3
-      start_period: 3s
+    smo:
+        restart: always
+        container_name: smo
+        build: .
+        env_file:
+            - config/flask.env
+        volumes:
+            - type: bind
+              source: ~/.kube/
+              target: /home/python/.kube
+              read_only: true
+            - type: bind
+              source: ~/.docker/
+              target: /home/python/.docker
+              read_only: true
+        depends_on:
+            postgres:
+                condition: service_healthy
+        networks:
+            - kind
+        ports:
+            - 8000:8000
+        extra_hosts:
+            - 'host.docker.internal:host-gateway'
+    postgres:
+        restart: always
+        container_name: postgres
+        image: postgres:16.2
+        networks:
+            - kind
+        env_file:
+            - config/postgres.env
+        healthcheck:
+            test:
+                [
+                    'CMD-SHELL',
+                    'pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB} -t 1',
+                ]
+            interval: 3s
+            timeout: 3s
+            retries: 3
+            start_period: 3s
+networks:
+    kind:
+        external: true
diff --git a/src/app.py b/src/app.py
index a2d0f3c..fb6868d 100644
--- a/src/app.py
+++ b/src/app.py
@@ -16,10 +16,10 @@ from routes.nfvcl.os_k8s import os_k8s
 from routes.nfvcl.vim import vim
 from services.cluster.cluster_service import fetch_clusters
 
-env = os.environ.get('FLASK_ENV', 'development')
+env = os.environ.get("FLASK_ENV", "development")
 
 
-def create_app(app_name='smo'):
+def create_app(app_name="smo"):
     """Function that returns a configured Flask app."""
 
     ROOT_PATH = os.path.dirname(__file__)
@@ -34,25 +34,24 @@ def create_app(app_name='smo'):
     app.register_blueprint(vim)
 
     app.register_error_handler(
-        subprocess.CalledProcessError,
-        error_handlers.handle_subprocess_error
-    )
-    app.register_error_handler(
-        yaml.YAMLError,
-        error_handlers.handle_yaml_read_error
+        subprocess.CalledProcessError, error_handlers.handle_subprocess_error
     )
+    app.register_error_handler(yaml.YAMLError, error_handlers.handle_yaml_read_error)
 
     db.init_app(app)
     with app.app_context():
         db.create_all()
         fetch_clusters(
-            app.config['KARMADA_KUBECONFIG'],
-            app.config['GRAFANA_HOST'], app.config['GRAFANA_USERNAME'], app.config['GRAFANA_PASSWORD']
+            karmada_kubeconfig=app.config["KARMADA_KUBECONFIG"],
+            grafana_host=app.config["GRAFANA_HOST"],
+            grafana_username=app.config["GRAFANA_USERNAME"],
+            grafana_password=app.config["GRAFANA_PASSWORD"],
+            karmada_context=app.config["KARMADA_CONTEXT"],
         )
 
     return app
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     app = create_app()
     app.run()
diff --git a/src/config.py b/src/config.py
index 628f2f9..88b7766 100644
--- a/src/config.py
+++ b/src/config.py
@@ -7,73 +7,77 @@ from dotenv import load_dotenv
 load_dotenv()
 
 SWAGGER_CONFIG = {
-    'title': 'SMO-API',
-    'uiversion': 3,
-    'specs_route': '/docs/',
-    'specs': [
+    "title": "SMO-API",
+    "uiversion": 3,
+    "specs_route": "/docs/",
+    "specs": [
         {
-            'endpoint': 'smo-api-spec',
-            'route': '/smo-api-spec.json',
-            'rule_filter': lambda rule: True,  # all in
-            'model_filter': lambda tag: True,  # all in
+            "endpoint": "smo-api-spec",
+            "route": "/smo-api-spec.json",
+            "rule_filter": lambda rule: True,  # all in
+            "model_filter": lambda tag: True,  # all in
         }
     ],
-    'ui_params': {
-        'apisSorter': 'alpha',
-        'operationsSorter': 'alpha',
-        'tagsSorter': 'alpha'
+    "ui_params": {
+        "apisSorter": "alpha",
+        "operationsSorter": "alpha",
+        "tagsSorter": "alpha",
     },
-    'ui_params_text': (
-        '{\n'
+    "ui_params_text": (
+        "{\n"
         '    "operationsSorter": (a, b) => {\n'
         '        var order = { "get": "0", "post": "1", "put": "2", "delete": "3" };\n'
         '        return order[a.get("method")].localeCompare(order[b.get("method")]);\n'
-        '    }\n'
-        '}'
-    )
+        "    }\n"
+        "}"
+    ),
 }
 
 
 def str_to_bool(str_variable):
-    return str_variable.lower() in ('t', 'true')
+    return str_variable.lower() in ("t", "true")
 
 
 class Config:
     """Database connection credentials."""
-    SQLALCHEMY_DATABASE_URI = 'postgresql://{}:{}@{}:5432/{}'.format(
-        os.getenv('DB_USER', 'root'),
-        os.getenv('DB_PASSWORD', 'password'),
-        os.getenv('DB_HOST', 'localhost'),
-        os.getenv('DB_NAME', 'smo')
+
+    SQLALCHEMY_DATABASE_URI = "postgresql://{}:{}@{}:5432/{}".format(
+        os.getenv("DB_USER", "root"),
+        os.getenv("DB_PASSWORD", "password"),
+        os.getenv("DB_HOST", "localhost"),
+        os.getenv("DB_NAME", "smo"),
     )
-    KARMADA_KUBECONFIG = '/home/python/.kube/{}'.format(
-        os.getenv('KARMADA_KUBECONFIG', 'karmada-apiserver.config')
+    KARMADA_KUBECONFIG = "/home/python/.kube/{}".format(
+        os.getenv("KARMADA_KUBECONFIG", "karmada-apiserver.config")
     )
-    NFVCL_BASE_URL = os.getenv('NFVCL_BASE_URL')
-    INSECURE_REGISTRY = str_to_bool(os.getenv('INSECURE_REGISTRY', 'True'))
-    PROMETHEUS_HOST = os.getenv('PROMETHEUS_HOST')
-    SCALING_INTERVAL = os.getenv('SCALING_INTERVAL')
-    GRAFANA_HOST = os.getenv('GRAFANA_HOST')
-    GRAFANA_USERNAME = os.getenv('GRAFANA_USERNAME')
-    GRAFANA_PASSWORD = os.getenv('GRAFANA_PASSWORD')
-    SCALING_ENABLED = str_to_bool(os.getenv('SCALING_ENABLED'))
+    KARMADA_CONTEXT = os.getenv("KARMADA_CONTEXT", "karmada-apiserver")
+    NFVCL_BASE_URL = os.getenv("NFVCL_BASE_URL")
+    INSECURE_REGISTRY = str_to_bool(os.getenv("INSECURE_REGISTRY", "True"))
+    PROMETHEUS_HOST = os.getenv("PROMETHEUS_HOST")
+    SCALING_INTERVAL = os.getenv("SCALING_INTERVAL")
+    GRAFANA_HOST = os.getenv("GRAFANA_HOST")
+    GRAFANA_USERNAME = os.getenv("GRAFANA_USERNAME")
+    GRAFANA_PASSWORD = os.getenv("GRAFANA_PASSWORD")
+    SCALING_ENABLED = str_to_bool(os.getenv("SCALING_ENABLED"))
     SWAGGER = SWAGGER_CONFIG
 
 
 class ProdConfig(Config):
     """Production settings."""
-    FLASK_ENV = 'production'
+
+    FLASK_ENV = "production"
     DEBUG = False
 
 
 class DevConfig(Config):
     """Development settings."""
-    FLASK_ENV = 'development'
+
+    FLASK_ENV = "development"
     DEBUG = True
 
 
 configs = {
-    'default': ProdConfig,
-    'production': ProdConfig,
-    'development': DevConfig,
+    "default": ProdConfig,
+    "production": ProdConfig,
+    "development": DevConfig,
 }
diff --git a/src/routes/cluster/cluster.py b/src/routes/cluster/cluster.py
index 6aef0b6..8358212 100644
--- a/src/routes/cluster/cluster.py
+++ b/src/routes/cluster/cluster.py
@@ -5,16 +5,19 @@ from flask import Blueprint, current_app
 
 from services.cluster.cluster_service import fetch_clusters
 
-cluster = Blueprint('cluster', __name__, url_prefix='/clusters')
+cluster = Blueprint("cluster", __name__, url_prefix="/clusters")
 
 
-@cluster.route('/', methods=['GET'])
-@swag_from('swagger/get_clusters.yaml')
+@cluster.route("/", methods=["GET"])
+@swag_from("swagger/get_clusters.yaml")
 def get_clusters():
     """Fetches all Bare-metal Kubernetes clusters."""
 
     clusters = fetch_clusters(
-        current_app.config['KARMADA_KUBECONFIG'],
-        current_app.config['GRAFANA_HOST'], current_app.config['GRAFANA_USERNAME'], current_app.config['GRAFANA_PASSWORD']
+        karmada_kubeconfig=app.config["KARMADA_KUBECONFIG"],
+        grafana_host=app.config["GRAFANA_HOST"],
+        grafana_username=app.config["GRAFANA_USERNAME"],
+        grafana_password=app.config["GRAFANA_PASSWORD"],
+        karmada_context=app.config["KARMADA_CONTEXT"],
     )
     return clusters, 200
diff --git a/src/routes/hdag/graph.py b/src/routes/hdag/graph.py
index df4daf2..c53a64d 100644
--- a/src/routes/hdag/graph.py
+++ b/src/routes/hdag/graph.py
@@ -4,23 +4,31 @@ import yaml
 from flasgger import swag_from
 from flask import Blueprint, request
 
-from services.hdag.graph_service import deploy_graph, fetch_graph, \
-    fetch_project_graphs, remove_graph, start_graph, stop_graph, \
-    trigger_placement, get_descriptor_from_artifact, deploy_conditional_service
+from services.hdag.graph_service import (
+    deploy_graph,
+    fetch_graph,
+    fetch_project_graphs,
+    remove_graph,
+    start_graph,
+    stop_graph,
+    trigger_placement,
+    get_descriptor_from_artifact,
+    deploy_conditional_service,
+)
 
-graph = Blueprint('graph', __name__)
+graph = Blueprint("graph", __name__)
 
 
-@graph.route('/project/<project>/graphs', methods=['GET'])
-@swag_from('swagger/get_all_graphs.yaml')
+@graph.route("/project/<project>/graphs", methods=["GET"])
+@swag_from("swagger/get_all_graphs.yaml")
 def get_all_graphs(project):
     """Fetches all graphs under a project."""
 
     return fetch_project_graphs(project), 200
 
 
-@graph.route('/project/<project>/graphs', methods=['POST'])
-@swag_from('swagger/deploy.yaml')
+@graph.route("/project/<project>/graphs", methods=["POST"])
+@swag_from("swagger/deploy.yaml")
 def deploy(project):
     """
     Handles the graph deployment. The input can either be an artifact
@@ -29,19 +37,34 @@ def deploy(project):
     """
 
     request_data = request.get_json()
-    if isinstance(request_data, dict) and 'artifact' in request_data:
-        artifact_ref = request_data['artifact']
+    import sys
+
+    print("//// request_data:", file=sys.stderr)
+    print(request_data, file=sys.stderr)
+    print("////////////////////////////////////////", file=sys.stderr)
+    if isinstance(request_data, dict) and "artifact" in request_data:
+        print(
+            'isinstance(request_data, dict) and "artifact" in request_data',
+            file=sys.stderr,
+        )
+        artifact_ref = request_data["artifact"]
+        print(f"/// artifact_ref: {artifact_ref!r}", file=sys.stderr)
         descriptor = get_descriptor_from_artifact(project, artifact_ref)
     else:
+        print(
+            'NOT isinstance(request_data, dict) and "artifact" in request_data',
+            file=sys.stderr,
+        )
         descriptor = yaml.safe_load(request_data)
-    graph_descriptor = descriptor['hdaGraph']
+    print(f"descriptor: {descriptor!r}", file=sys.stderr)
+    graph_descriptor = descriptor["hdaGraph"]
     deploy_graph(project, graph_descriptor)
 
-    return 'Graph deployment successful\n', 200
+    return "Graph deployment successful\n", 200
 
 
-@graph.route('/graphs/<name>', methods=['GET'])
-@swag_from('swagger/get_graph.yaml')
+@graph.route("/graphs/<name>", methods=["GET"])
+@swag_from("swagger/get_graph.yaml")
 def get_graph(name):
     """Retrieves an application graph descriptor."""
 
@@ -50,50 +73,50 @@ def get_graph(name):
     if graph is not None:
         return graph.to_dict(), 200
     else:
-        return f'Graph with name {name} not found\n', 404
+        return f"Graph with name {name} not found\n", 404
 
 
-@graph.route('/graphs/<name>/placement', methods=['GET'])
-@swag_from('swagger/placement.yaml')
+@graph.route("/graphs/<name>/placement", methods=["GET"])
+@swag_from("swagger/placement.yaml")
 def placement(name):
     """Runs the placement algorithm on the graph."""
 
     trigger_placement(name)
 
-    return f'Placement of graph {name} triggered\n', 200
+    return f"Placement of graph {name} triggered\n", 200
 
 
-@graph.route('/graphs/<name>/start', methods=['GET'])
-@swag_from('swagger/start.yaml')
+@graph.route("/graphs/<name>/start", methods=["GET"])
+@swag_from("swagger/start.yaml")
 def start(name):
     """Starts a stopped graph."""
 
     start_graph(name)
 
-    return 'Graph stopped\n', 200
+    return "Graph stopped\n", 200
 
 
-@graph.route('/graphs/<name>/stop', methods=['GET'])
-@swag_from('swagger/stop.yaml')
+@graph.route("/graphs/<name>/stop", methods=["GET"])
+@swag_from("swagger/stop.yaml")
 def stop(name):
     """Uninstalls graphs artifacts without erasing from the database."""
 
     stop_graph(name)
 
-    return 'Graph stopped\n', 200
+    return "Graph stopped\n", 200
 
 
-@graph.route('/graphs/<name>', methods=['DELETE'])
-@swag_from('swagger/remove.yaml')
+@graph.route("/graphs/<name>", methods=["DELETE"])
+@swag_from("swagger/remove.yaml")
 def remove(name):
     """Handles the graph removal."""
 
     remove_graph(name)
 
-    return 'Removal successful\n', 200
+    return "Removal successful\n", 200
 
 
-@graph.route('/alerts', methods=['POST'])
+@graph.route("/alerts", methods=["POST"])
 def alert():
     """Deploys a service that is triggered when an alert has been fired."""
     data = request.get_json()
diff --git a/src/services/cluster/cluster_service.py b/src/services/cluster/cluster_service.py
index 6777d7f..ac8f7a8 100644
--- a/src/services/cluster/cluster_service.py
+++ b/src/services/cluster/cluster_service.py
@@ -5,34 +5,40 @@ from utils.grafana_helper import GrafanaHelper
 from utils.karmada_helper import KarmadaHelper
 
 
-def fetch_clusters(karmada_kubeconfig, grafana_host, grafana_username, grafana_password):
+def fetch_clusters(
+    karmada_kubeconfig="",
+    grafana_host="",
+    grafana_username="",
+    grafana_password="",
+    karmada_context="",
+):
     """Retrieves all cluster data."""
 
     cluster_dict = []
 
-    karmada_helper = KarmadaHelper(karmada_kubeconfig)
+    karmada_helper = KarmadaHelper(karmada_kubeconfig, karmada_context=karmada_context)
     karmada_cluster_info = karmada_helper.get_cluster_info()
 
-
-
     for cluster_name, info in karmada_cluster_info.items():
         cluster = db.session.query(Cluster).filter(Cluster.name == cluster_name).first()
         if cluster is not None:
-            cluster.available_cpu = info['remaining_cpu']
-            cluster.available_ram = info['remaining_memory_bytes']
+            cluster.available_cpu = info["remaining_cpu"]
+            cluster.available_ram = info["remaining_memory_bytes"]
         else:
-            grafana_helper = GrafanaHelper(grafana_host, grafana_username, grafana_password)
+            grafana_helper = GrafanaHelper(
+                grafana_host, grafana_username, grafana_password
+            )
             dashboard = grafana_helper.create_cluster_dashboard(cluster_name)
             response = grafana_helper.publish_dashboard(dashboard)
-            grafana_url = f'{grafana_host}{response["url"]}'
+            grafana_url = f"{grafana_host}{response['url']}"
             cluster = Cluster(
                 name=cluster_name,
-                location='Unknown',
-                available_cpu=info['remaining_cpu'],
-                available_ram=info['remaining_memory_bytes'],
-                availability=info['availability'],
+                location="Unknown",
+                available_cpu=info["remaining_cpu"],
+                available_ram=info["remaining_memory_bytes"],
+                availability=info["availability"],
                 acceleration=0,
-                grafana=grafana_url
+                grafana=grafana_url,
             )
             db.session.add(cluster)
         db.session.commit()
diff --git a/src/services/hdag/graph_service.py b/src/services/hdag/graph_service.py
index e985018..553e190 100644
--- a/src/services/hdag/graph_service.py
+++ b/src/services/hdag/graph_service.py
@@ -13,7 +13,12 @@ from models import db, Cluster, Graph, Service
 from utils.grafana_helper import GrafanaHelper
 from utils.karmada_helper import KarmadaHelper
 from utils.prometheus_helper import PrometheusHelper
-from utils.placement import convert_placement, decide_placement, swap_placement, calculate_naive_placement
+from utils.placement import (
+    convert_placement,
+    decide_placement,
+    swap_placement,
+    calculate_naive_placement,
+)
 from utils.scaling import scaling_loop
 from utils.intent_translation import tranlsate_cpu, tranlsate_memory, tranlsate_storage
 
@@ -38,24 +43,25 @@ def deploy_graph(project, graph_descriptor):
     """
 
     grafana_helper = GrafanaHelper(
-        current_app.config['GRAFANA_HOST'], current_app.config['GRAFANA_USERNAME'],
-        current_app.config['GRAFANA_PASSWORD']
+        current_app.config["GRAFANA_HOST"],
+        current_app.config["GRAFANA_USERNAME"],
+        current_app.config["GRAFANA_PASSWORD"],
     )
-    prom_helper = PrometheusHelper(current_app.config['PROMETHEUS_HOST'])
+    prom_helper = PrometheusHelper(current_app.config["PROMETHEUS_HOST"])
 
     hdag_config = graph_descriptor
-    name = hdag_config['id']
+    name = hdag_config["id"]
 
     graph = db.session.query(Graph).filter_by(name=name).first()
     if graph is not None:
-        raise BadRequest(f'Graph with name {name} already exists')
+        raise BadRequest(f"Graph with name {name} already exists")
 
     graph = Graph(
         name=name,
         graph_descriptor=graph_descriptor,
         project=project,
-        status='Running',
-        grafana=None
+        status="Running",
+        grafana=None,
     )
     db.session.add(graph)
     db.session.commit()
@@ -63,22 +69,38 @@ def deploy_graph(project, graph_descriptor):
     background_scaling_threads[name] = [None, None]
     stop_events[name] = [threading.Event(), threading.Event()]
 
-    services = hdag_config['services']
-    cpu_limits = [tranlsate_cpu(service['deployment']['intent']['compute']['cpu']) for service in services]
-    acceleration_list = [1 if service['deployment']['intent']['compute']['gpu']['enabled'] == 'True' else 0 for service in services]
+    services = hdag_config["services"]
+    cpu_limits = [
+        tranlsate_cpu(service["deployment"]["intent"]["compute"]["cpu"])
+        for service in services
+    ]
+    acceleration_list = [
+        1
+        if service["deployment"]["intent"]["compute"]["gpu"]["enabled"] == "True"
+        else 0
+        for service in services
+    ]
     replicas = [1 for _ in services]
 
     available_clusters = db.session.query(Cluster).filter_by(availability=True)
     cluster_list = [cluster.name for cluster in available_clusters]
-    cluster_capacity = {cluster.name: cluster.available_cpu for cluster in available_clusters}
-    cluster_acceleration = {cluster.name: cluster.acceleration for cluster in available_clusters}
+    cluster_capacity = {
+        cluster.name: cluster.available_cpu for cluster in available_clusters
+    }
+    cluster_acceleration = {
+        cluster.name: cluster.acceleration for cluster in available_clusters
+    }
     cluster_capacity_list = [value for value in cluster_capacity.values()]
     cluster_acceleration_list = [value for value in cluster_acceleration.values()]
 
     service_placement = {}
-    if not hdag_config['hdaGraphIntent']['useStaticPlacement']:
+    if not hdag_config["hdaGraphIntent"]["useStaticPlacement"]:
         placement = calculate_naive_placement(
-            cluster_capacity_list, cluster_acceleration_list, cpu_limits, acceleration_list, replicas
+            cluster_capacity_list,
+            cluster_acceleration_list,
+            cpu_limits,
+            acceleration_list,
+            replicas,
         )
         graph.placement = placement
 
@@ -89,53 +111,63 @@ def deploy_graph(project, graph_descriptor):
     svc_names = []
     for service in services:
         alert = {}
-        name = service['id']
+        name = service["id"]
         svc_names.append(name)
-        artifact = service['artifact']
-        artifact_ref = artifact['ociImage']
-        implementer = artifact['ociConfig']['implementer']
-        artifact_type = artifact['ociConfig']['type']
-        values_overwrite = artifact['valuesOverwrite']
+        artifact = service["artifact"]
+        artifact_ref = artifact["ociImage"]
+        implementer = artifact["ociConfig"]["implementer"]
+        artifact_type = artifact["ociConfig"]["type"]
+        values_overwrite = artifact["valuesOverwrite"]
         placement_dict = values_overwrite
 
         conditional_deployment = False
-        deployment_trigger = service['deployment']['trigger']
-        if 'event' in deployment_trigger:
+        deployment_trigger = service["deployment"]["trigger"]
+        if "event" in deployment_trigger:
             conditional_deployment = True
-            deployment_condition = deployment_trigger['event']['condition']
-            events = deployment_trigger['event']['events']
+            deployment_condition = deployment_trigger["event"]["condition"]
+            events = deployment_trigger["event"]["events"]
             for event in events:
-                sources = event['source']
-                event_id = event['id']
-                event_condition = event['condition']
-                prom_query = event_condition['promQuery']
-                grace_period = event_condition['gracePeriod']
-                description = event_condition['description']
+                sources = event["source"]
+                event_id = event["id"]
+                event_condition = event["condition"]
+                prom_query = event_condition["promQuery"]
+                grace_period = event_condition["gracePeriod"]
+                description = event_condition["description"]
 
-                alert = create_alert(event_id, prom_query, grace_period, description, name)
-                prom_helper.update_alert_rules(alert, 'add')
+                alert = create_alert(
+                    event_id, prom_query, grace_period, description, name
+                )
+                prom_helper.update_alert_rules(alert, "add")
 
-        cpu = tranlsate_cpu(service['deployment']['intent']['compute']['cpu'])
-        memory = tranlsate_memory(service['deployment']['intent']['compute']['ram'])
-        storage = tranlsate_storage(service['deployment']['intent']['compute']['storage'])
-        gpu = 1 if service['deployment']['intent']['compute']['gpu']['enabled'] == 'True' else 0
+        cpu = tranlsate_cpu(service["deployment"]["intent"]["compute"]["cpu"])
+        memory = tranlsate_memory(service["deployment"]["intent"]["compute"]["ram"])
+        storage = tranlsate_storage(
+            service["deployment"]["intent"]["compute"]["storage"]
+        )
+        gpu = (
+            1
+            if service["deployment"]["intent"]["compute"]["gpu"]["enabled"] == "True"
+            else 0
+        )
 
-        if not hdag_config['hdaGraphIntent']['useStaticPlacement']:
-            if implementer == 'WOT':
-                if 'voChartOverwrite' not in values_overwrite:
-                    values_overwrite['voChartOverwrite'] = {}
-                placement_dict = values_overwrite['voChartOverwrite']
+        if not hdag_config["hdaGraphIntent"]["useStaticPlacement"]:
+            if implementer == "WOT":
+                if "voChartOverwrite" not in values_overwrite:
+                    values_overwrite["voChartOverwrite"] = {}
+                placement_dict = values_overwrite["voChartOverwrite"]
 
-            placement_dict['clustersAffinity'] = [service_placement[name]]
-            placement_dict['serviceImportClusters'] = import_clusters[name]
+            placement_dict["clustersAffinity"] = [service_placement[name]]
+            placement_dict["serviceImportClusters"] = import_clusters[name]
 
-        status = 'Pending' if conditional_deployment else 'Deployed'
+        status = "Pending" if conditional_deployment else "Deployed"
 
         svc_dashboard = grafana_helper.create_graph_service(name)
         response = grafana_helper.publish_dashboard(svc_dashboard)
-        grafana_url = f'{current_app.config["GRAFANA_HOST"]}{response["url"]}'
+        grafana_url = f"{current_app.config['GRAFANA_HOST']}{response['url']}"
 
-        cluster_affinity = service_placement[name] if name in service_placement else None
+        cluster_affinity = (
+            service_placement[name] if name in service_placement else None
+        )
         svc = Service(
             name=name,
             values_overwrite=values_overwrite,
@@ -150,22 +182,23 @@ def deploy_graph(project, graph_descriptor):
             storage=storage,
             gpu=gpu,
             grafana=grafana_url,
-            alert=alert
+            alert=alert,
         )
         db.session.add(svc)
 
         if not conditional_deployment:
-            helm_install_artifact(name, artifact_ref, values_overwrite, graph.project, 'install')
-
+            helm_install_artifact(
+                name, artifact_ref, values_overwrite, graph.project, "install"
+            )
 
     dashboard = grafana_helper.create_graph_dashboard(graph.name, svc_names)
     response = grafana_helper.publish_dashboard(dashboard)
-    grafana_url = f'{current_app.config["GRAFANA_HOST"]}{response["url"]}'
+    grafana_url = f"{current_app.config['GRAFANA_HOST']}{response['url']}"
     graph.grafana = grafana_url
 
     db.session.commit()
 
-    if current_app.config['SCALING_ENABLED']:
+    if current_app.config["SCALING_ENABLED"]:
         spawn_scaling_processes(graph.name, cluster_placement)
 
 
@@ -182,11 +215,14 @@ def trigger_placement(name):
 
     graph = db.session.query(Graph).filter_by(name=name).first()
     if graph is None:
-        raise NotFound(f'Graph with name {name} not found')
+        raise NotFound(f"Graph with name {name} not found")
 
     for stop_event in stop_events[name]:
         stop_event.set()
-    karmada_helper = KarmadaHelper(current_app.config['KARMADA_KUBECONFIG'])
+    karmada_helper = KarmadaHelper(
+        current_app.config["KARMADA_KUBECONFIG"],
+        karmada_context=current_app.config["KARMADA_CONTEXT"],
+    )
 
     services = [service.name for service in graph.services]
     cpu_limits = [service.cpu for service in graph.services]
@@ -195,18 +231,26 @@ def trigger_placement(name):
 
     available_clusters = db.session.query(Cluster).filter_by(availability=True)
     cluster_list = [cluster.name for cluster in available_clusters]
-    cluster_capacity = {cluster.name: cluster.available_cpu for cluster in available_clusters}
-    cluster_acceleration = {cluster.name: cluster.acceleration for cluster in available_clusters}
+    cluster_capacity = {
+        cluster.name: cluster.available_cpu for cluster in available_clusters
+    }
+    cluster_acceleration = {
+        cluster.name: cluster.acceleration for cluster in available_clusters
+    }
     cluster_capacity_list = [value for value in cluster_capacity.values()]
     cluster_acceleration_list = [value for value in cluster_acceleration.values()]
 
     placement = decide_placement(
-        cluster_capacity_list, cluster_acceleration_list, cpu_limits,
-        acceleration_list, current_replicas, graph.placement
+        cluster_capacity_list,
+        cluster_acceleration_list,
+        cpu_limits,
+        acceleration_list,
+        current_replicas,
+        graph.placement,
     )
     graph.placement = placement
     db.session.commit()
-    descriptor_services = graph.graph_descriptor['services']
+    descriptor_services = graph.graph_descriptor["services"]
 
     available_clusters = db.session.query(Cluster).filter_by(availability=True)
     cluster_list = [cluster.name for cluster in available_clusters]
@@ -219,19 +263,25 @@ def trigger_placement(name):
         # Updating JSONB fields requires new dictionary creation
         values_overwrite = dict(service.values_overwrite)
         placement_dict = values_overwrite
-        if service.artifact_implementer == 'WOT':
-            if 'voChartOverwrite' not in values_overwrite:
-                values_overwrite['voChartOverwrite'] = {}
-            placement_dict = values_overwrite['voChartOverwrite']
-        if placement_dict['clustersAffinity'][0] != service_placement[service.name]:
-            placement_dict['clustersAffinity'] = [service_placement[service.name]]
-            placement_dict['serviceImportClusters'] = import_clusters[service.name]
+        if service.artifact_implementer == "WOT":
+            if "voChartOverwrite" not in values_overwrite:
+                values_overwrite["voChartOverwrite"] = {}
+            placement_dict = values_overwrite["voChartOverwrite"]
+        if placement_dict["clustersAffinity"][0] != service_placement[service.name]:
+            placement_dict["clustersAffinity"] = [service_placement[service.name]]
+            placement_dict["serviceImportClusters"] = import_clusters[service.name]
             service.values_overwrite = values_overwrite
             db.session.commit()
 
-            helm_install_artifact(service.name, service.artifact_ref, values_overwrite, graph.project, 'upgrade')
+            helm_install_artifact(
+                service.name,
+                service.artifact_ref,
+                values_overwrite,
+                graph.project,
+                "upgrade",
+            )
 
-    if current_app.config['SCALING_ENABLED']:
+    if current_app.config["SCALING_ENABLED"]:
         spawn_scaling_processes(name, cluster_placement)
 
 
@@ -240,21 +290,21 @@ def start_graph(name):
 
     graph = db.session.query(Graph).filter_by(name=name).first()
     if graph is None:
-        raise NotFound(f'Graph with name {name} not found')
-    if graph.status == 'Running':
-        raise BadRequest(f'Graph with name {name} is already running')
-    graph.status = 'Running'
+        raise NotFound(f"Graph with name {name} not found")
+    if graph.status == "Running":
+        raise BadRequest(f"Graph with name {name} is already running")
+    graph.status = "Running"
     for service in graph.services:
         if service.alert != {}:
-            prom_helper = PrometheusHelper(current_app.config['PROMETHEUS_HOST'])
-            prom_helper.update_alert_rules(service.alert, 'add')
-        if service.status == 'Deployed':
+            prom_helper = PrometheusHelper(current_app.config["PROMETHEUS_HOST"])
+            prom_helper.update_alert_rules(service.alert, "add")
+        if service.status == "Deployed":
             helm_install_artifact(
                 service.name,
                 service.artifact_ref,
                 service.values_overwrite,
                 graph.project,
-                'install'
+                "install",
             )
     db.session.commit()
 
@@ -264,17 +314,17 @@ def stop_graph(name):
 
     graph = db.session.query(Graph).filter_by(name=name).first()
     if graph is None:
-        raise NotFound(f'Graph with name {name} not found')
-    if graph.status == 'Stopped':
-        raise BadRequest(f'Graph with name {name} is already stopped')
+        raise NotFound(f"Graph with name {name} not found")
+    if graph.status == "Stopped":
+        raise BadRequest(f"Graph with name {name} is already stopped")
 
     helm_uninstall_graph(graph.services, graph.project)
     for stop_event in stop_events[name]:
         stop_event.set()
 
-    graph.status = 'Stopped'
+    graph.status = "Stopped"
     for service in graph.services:
-        service.status = 'Not deployed'
+        service.status = "Not deployed"
     db.session.commit()
 
 
@@ -283,10 +333,10 @@ def remove_graph(name):
 
     graph = db.session.query(Graph).filter_by(name=name).first()
     if graph is None:
-        raise NotFound(f'Graph with name {name} not found')
+        raise NotFound(f"Graph with name {name} not found")
 
     helm_uninstall_graph(graph.services, graph.project)
-    #for stop_event in stop_events[name]:
+    # for stop_event in stop_events[name]:
     #    stop_event.set()
 
     db.session.delete(graph)
@@ -296,20 +346,26 @@ def remove_graph(name):
 def deploy_conditional_service(data):
     """Deploys a service that is triggered when an alert has been fired."""
 
-    alerts = data['alerts']
+    alerts = data["alerts"]
     for alert in alerts:
-        labels = alert['labels']
-        if 'service' in labels:
-            alertname = labels['alertname']
-            service_name = labels['service']
+        labels = alert["labels"]
+        if "service" in labels:
+            alertname = labels["alertname"]
+            service_name = labels["service"]
             service = db.session.query(Service).filter_by(name=service_name).first()
             graph = service.graph
             if service is None:
                 continue
 
-            helm_install_artifact(service.name, service.artifact_ref, service.values_overwrite, graph.project, 'install')
+            helm_install_artifact(
+                service.name,
+                service.artifact_ref,
+                service.values_overwrite,
+                graph.project,
+                "install",
+            )
 
-            service.status = 'Deployed'
+            service.status = "Deployed"
             db.session.commit()
 
 
@@ -322,16 +378,14 @@ def create_service_imports(services, service_placement):
     """
 
     # For each service a list of clusters to which its service will be imported
-    service_import_clusters = {
-        service['id']: [] for service in services
-    }
+    service_import_clusters = {service["id"]: [] for service in services}
 
     for service in services:
-        connection_points = service['deployment']['intent']['connectionPoints']
+        connection_points = service["deployment"]["intent"]["connectionPoints"]
         for other_service in services:
-            if other_service['id'] in connection_points:
-                service_import_clusters[other_service['id']].extend(
-                    [service_placement[service['id']]]
+            if other_service["id"] in connection_points:
+                service_import_clusters[other_service["id"]].extend(
+                    [service_placement[service["id"]]]
                 )
 
     return service_import_clusters
@@ -342,21 +396,165 @@ def get_descriptor_from_artifact(project, artifact_ref):
     Calls the hdarctl cli to pull an artifact and deploys
     the descriptor inside after untaring the artifact.
     """
+    #
+    # /// artifact_ref: 'http://host.docker.internal:5000/test/image-detection-graph'
+    # smo  | hdarctl failed: 1
+    # smo  | stdout:
+    # smo  |
+    # smo  |
+    # smo  | stderr
+    # smo  | Error: Unable to create repo
+    # smo  |
+
+    # BUT
+    # curl -X GET http://localhost:5000/v2/_catalog
+    # {"repositories":["custom-vo","image-detection","noise-reduction","test/image-compression-vo","test/image-detection","test/image-detection-graph","test/noise-reduction"]}
+
+    import sys
 
     with tempfile.TemporaryDirectory() as dirpath:
-        subprocess.run([
-            'hdarctl',
-            'pull',
-            artifact_ref,
-            '--untar',
-            '--destination',
-            dirpath
-        ])
-
-        for (root, dirs, files) in walk(dirpath):
+        print(f"hdarctl artifact_ref:{artifact_ref!r}", file=sys.stderr)
+        try:
+            # subprocess.run(
+            #     ["mkdir", f"{dirpath}/toto"],
+            #     capture_output=True,
+            #     text=True,
+            #     check=True,
+            # )
+            res = subprocess.run(
+                [
+                    "curl",
+                    "-s",
+                    "-X",
+                    "GET",
+                    "http://host.docker.internal:5000/v2/_catalog",
+                ],
+                capture_output=True,
+                text=True,
+                check=True,
+            )
+            print("curl stdout:", file=sys.stderr)
+            print(res.stdout, file=sys.stderr)
+            print("curl stderr", file=sys.stderr)
+            print(res.stderr, file=sys.stderr)
+
+            res = subprocess.run(
+                [
+                    "curl",
+                    "-s",
+                    "-X",
+                    "GET",
+                    "http://host.docker.internal:5000/v2/_catalog",
+                ],
+                capture_output=True,
+                text=True,
+                check=True,
+            )
+            print("curl stdout:", file=sys.stderr)
+            print(res.stdout, file=sys.stderr)
+            print("curl stderr", file=sys.stderr)
+            print(res.stderr, file=sys.stderr)
+
+            res = subprocess.run(
+                [
+                    "curl",
+                    "-s",
+                    "http://host.docker.internal:5000/v2/test/noise-reduction/tags/list",
+                ],
+                capture_output=True,
+                text=True,
+                check=True,
+            )
+            print("curl tags stdout:", file=sys.stderr)
+            print(res.stdout, file=sys.stderr)
+            print("curl tags stderr", file=sys.stderr)
+            print(res.stderr, file=sys.stderr)
+
+            res = subprocess.run(
+                [
+                    "curl",
+                    "-s",
+                    "-H",
+                    "Accept: application/vnd.docker.distribution.manifest.v2+json, application/vnd.oci.image.manifest.v1+json",
+                    "http://host.docker.internal:5000/v2/test/noise-reduction/manifests/0.1.0",
+                ],
+                capture_output=True,
+                text=True,
+                check=True,
+            )
+            print("curl manifests stdout:", file=sys.stderr)
+
+            # smo  | {"schemaVersion":2,"mediaType":"application/vnd.oci.image.manifest.v1+json","config":{"mediaType":"application/vnd.cncf.helm.config.v1+json","digest":"sha256:371960a204fe0618848b37c46b78052776c9c957b587edbd38dc190dd6c45e67","size":241},"layers":[{"mediaType":"application/vnd.cncf.helm.chart.content.v1.tar+gzip","digest":"sha256:9ec6a09bf7268a4a55a7a71f649d08e05bb709297d19aa81a0191036e888ba0e","size":1621,"annotations":{"org.opencontainers.image.title":"noise-reduction-0.1.0.tar.gz"}}],"annotations":{"org.opencontainers.image.created":"2025-08-04T15:19:35Z"}}
+
+            print(res.stdout, file=sys.stderr)
+            print("curl manifests stderr", file=sys.stderr)
+            print(res.stderr, file=sys.stderr)
+
+            res = subprocess.run(
+                [
+                    "curl",
+                    "-o",
+                    "noise-reduction-0.1.0.tar.gz",
+                    "http://host.docker.internal:5000/v2/test/noise-reduction/blobs/sha256:9ec6a09bf7268a4a55a7a71f649d08e05bb709297d19aa81a0191036e888ba0e",
+                ],
+                capture_output=True,
+                text=True,
+                check=True,
+            )
+            print("curl pull file stdout:", file=sys.stderr)
+            print(res.stdout, file=sys.stderr)
+            print("curl pull stderr", file=sys.stderr)
+            print(res.stderr, file=sys.stderr)
+
+            # subprocess.run(
+            #     # ["hdarctl", "pull", artifact_ref, "--untar", "--destination", dirpath],
+            #     ["chmod", "a+rwx", dirpath],
+            #     capture_output=True,
+            #     text=True,
+            #     check=True,
+            # )
+
+            # subprocess.run(
+            #     # ["hdarctl", "pull", artifact_ref, "--untar", "--destination", dirpath],
+            #     [
+            #         "hdarctl",
+            #         # "pull",
+            #         # artifact_ref,
+            #         "--help",
+            #     ],
+            #     capture_output=True,
+            #     text=True,
+            #     check=True,
+            # )
+            # print("hdarctl help stdout:", file=sys.stderr)
+            # print(res.stdout, file=sys.stderr)
+            # print("hdarctl help stderr", file=sys.stderr)
+            # print(res.stderr, file=sys.stderr)
+
+            subprocess.run(
+                # ["hdarctl", "pull", artifact_ref, "--untar", "--destination", dirpath],
+                ["hdarctl", "pull", artifact_ref, "-d", dirpath],
+                capture_output=True,
+                text=True,
+                check=True,
+            )
+            print("hdarctl pull stdout:", file=sys.stderr)
+            print(res.stdout, file=sys.stderr)
+            print("hdarctl pull stderr", file=sys.stderr)
+            print(res.stderr, file=sys.stderr)
+        except Exception as e:
+            print(f"hdarctl failed: {e.returncode}", file=sys.stderr)
+            print("stdout:", file=sys.stderr)
+            print(e.stdout, file=sys.stderr)
+            print("stderr", file=sys.stderr)
+            print(e.stderr, file=sys.stderr)
+
+        for root, dirs, files in walk(dirpath):
+            print(f"files: {root!r} {dirs!r} {files!r}", file=sys.stderr)
             for file in files:
-                if file.endswith('.yaml') or file.endswith('.yml'):
-                    with open(path.join(root, file), 'r') as yaml_file:
+                print(f"file: {file!r}", file=sys.stderr)
+                if file.endswith(".yaml") or file.endswith(".yml"):
+                    with open(path.join(root, file), "r") as yaml_file:
                         data = yaml.safe_load(yaml_file)
                         return data
 
@@ -364,26 +562,26 @@ def get_descriptor_from_artifact(project, artifact_ref):
 def helm_install_artifact(name, artifact_ref, values_overwrite, namespace, command):
     """Executes helm command (install/upgrade) for artifact."""
 
-    with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml') as values_file:
+    with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml") as values_file:
         yaml.dump(values_overwrite, values_file)
 
         subprocess_arguments = [
-            'helm',
+            "helm",
             command,
             name,
             artifact_ref,
-            '--values',
+            "--values",
             values_file.name,
-            '--namespace',
+            "--namespace",
             namespace,
-            '--create-namespace',
-            '--kubeconfig',
-            current_app.config['KARMADA_KUBECONFIG']
+            "--create-namespace",
+            "--kubeconfig",
+            current_app.config["KARMADA_KUBECONFIG"],
         ]
-        if current_app.config['INSECURE_REGISTRY']:
-            subprocess_arguments.append('--plain-http')
-        if command == 'upgrade':
-            subprocess_arguments.append('--reuse-values')
+        if current_app.config["INSECURE_REGISTRY"]:
+            subprocess_arguments.append("--plain-http")
+        if command == "upgrade":
+            subprocess_arguments.append("--reuse-values")
         subprocess.run(subprocess_arguments)
 
 
@@ -392,32 +590,54 @@ def helm_uninstall_graph(services, namespace):
 
     for service in services:
         if service.alert != {}:
-            prom_helper = PrometheusHelper(current_app.config['PROMETHEUS_HOST'])
-            prom_helper.update_alert_rules(service.alert, 'remove')
-        if service.status == 'Deployed':
-            subprocess.run([
-                'helm',
-                'uninstall',
-                service.name,
-                '--namespace',
-                namespace,
-                '--kubeconfig',
-                current_app.config['KARMADA_KUBECONFIG']
-            ])
+            prom_helper = PrometheusHelper(current_app.config["PROMETHEUS_HOST"])
+            prom_helper.update_alert_rules(service.alert, "remove")
+        if service.status == "Deployed":
+            subprocess.run(
+                [
+                    "helm",
+                    "uninstall",
+                    service.name,
+                    "--namespace",
+                    namespace,
+                    "--kubeconfig",
+                    current_app.config["KARMADA_KUBECONFIG"],
+                ]
+            )
 
 
 def spawn_scaling_processes(graph_name, cluster_placement):
     """Spawns background threads that periodically run the scaling algorithm."""
 
-    MAXIMUM_REPLICAS = {'image-compression-vo': 3,'noise-reduction': 3,'image-detection': 3}
-    ACCELERATION = {'image-compression-vo': 0,'noise-reduction': 0,'image-detection': 0}
-    ALPHA = {'image-compression-vo': 33.33,'noise-reduction': 0.533,'image-detection': 1.67}
-    BETA = {'image-compression-vo': -16.66,'noise-reduction': -0.416,'image-detection': -0.01}
+    MAXIMUM_REPLICAS = {
+        "image-compression-vo": 3,
+        "noise-reduction": 3,
+        "image-detection": 3,
+    }
+    ACCELERATION = {
+        "image-compression-vo": 0,
+        "noise-reduction": 0,
+        "image-detection": 0,
+    }
+    ALPHA = {
+        "image-compression-vo": 33.33,
+        "noise-reduction": 0.533,
+        "image-detection": 1.67,
+    }
+    BETA = {
+        "image-compression-vo": -16.66,
+        "noise-reduction": -0.416,
+        "image-detection": -0.01,
+    }
 
     available_clusters = db.session.query(Cluster).filter_by(availability=True)
     cluster_list = [cluster.name for cluster in available_clusters]
-    cluster_capacity = {cluster.name: cluster.available_cpu for cluster in available_clusters}
-    cluster_acceleration = {cluster.name: cluster.acceleration for cluster in available_clusters}
+    cluster_capacity = {
+        cluster.name: cluster.available_cpu for cluster in available_clusters
+    }
+    cluster_acceleration = {
+        cluster.name: cluster.acceleration for cluster in available_clusters
+    }
 
     for cluster_index, cluster in enumerate(cluster_list):
         if cluster not in cluster_placement.keys():
@@ -441,11 +661,11 @@ def spawn_scaling_processes(graph_name, cluster_placement):
                 cluster_acceleration[cluster],
                 maximum_replicas,
                 managed_services,
-                current_app.config['SCALING_INTERVAL'],
-                current_app.config['KARMADA_KUBECONFIG'],
-                current_app.config['PROMETHEUS_HOST'],
-                stop_events[cluster_index]
-            )
+                current_app.config["SCALING_INTERVAL"],
+                current_app.config["KARMADA_KUBECONFIG"],
+                current_app.config["PROMETHEUS_HOST"],
+                stop_events[cluster_index],
+            ),
         )
         background_scaling_threads[cluster_index].daemon = True
         background_scaling_threads[cluster_index].start()
@@ -453,15 +673,9 @@ def spawn_scaling_processes(graph_name, cluster_placement):
 
 def create_alert(event_id, prom_query, grace_period, description, name):
     return {
-        'alert': f'{event_id}',
-        'annotations': {
-            'description': description,
-            'summary': description
-        },
-        'expr': f'{prom_query}',
-        'for': f'{grace_period}',
-        'labels': {
-            'severity': 'critical',
-            'service': name
-        }
+        "alert": f"{event_id}",
+        "annotations": {"description": description, "summary": description},
+        "expr": f"{prom_query}",
+        "for": f"{grace_period}",
+        "labels": {"severity": "critical", "service": name},
     }
diff --git a/src/utils/karmada_helper.py b/src/utils/karmada_helper.py
index d8775e9..17d451a 100644
--- a/src/utils/karmada_helper.py
+++ b/src/utils/karmada_helper.py
@@ -1,67 +1,93 @@
 """Karmada helper class and utility functions."""
 
-
 from kubernetes import client, config
 from kubernetes.utils import parse_quantity
 
 from utils.helpers import format_memory
 
 
-class KarmadaHelper():
+class KarmadaHelper:
     """Karmada helper class."""
 
-    def __init__(self, config_file_path, namespace='default'):
+    def __init__(self, config_file_path, namespace="default", karmada_context=""):
         self.namespace = namespace
         self.config_file_path = config_file_path
 
-        config.load_kube_config(config_file=self.config_file_path)
+        if karmada_context:
+            try:
+                config.load_kube_config(
+                    config_file=self.config_file_path, context=karmada_context
+                )
+                print(
+                    f"Successfully loaded kubeconfig for context: {karmada_context!r}"
+                )
+            except config.ConfigException as e:
+                msg = (
+                    "ERROR: Failed to load Karmada kubeconfig "
+                    f"context {karmada_context!r}: {e}\n"
+                    f"Check ~/.kube/{config_file_path} file has "
+                    f"the {karmada_context!r} context."
+                )
+                print(msg)
+                raise
+        else:
+            config.load_kube_config(config_file=self.config_file_path)
 
         self.custom_api = client.CustomObjectsApi()
         self.v1_api_client = client.AppsV1Api()
 
-
     def get_cluster_info(self):
-        group = 'cluster.karmada.io'
-        version = 'v1alpha1'
-        plural = 'clusters'
+        group = "cluster.karmada.io"
+        version = "v1alpha1"
+        plural = "clusters"
 
         clusters = self.custom_api.list_cluster_custom_object(group, version, plural)
 
         result = {}
-        for cluster in clusters['items']:
-            cluster_name = cluster['metadata']['name']
-            allocatable = cluster['status']['resourceSummary']['allocatable']
-            allocated = cluster['status']['resourceSummary']['allocated']
+        for cluster in clusters["items"]:
+            cluster_name = cluster["metadata"]["name"]
+            print(f"Found cluster: {cluster_name!r}")
+            allocatable = cluster["status"]["resourceSummary"]["allocatable"]
+            allocated = cluster["status"]["resourceSummary"]["allocated"]
 
-            total_cpu = parse_quantity(allocatable['cpu'])
-            allocated_cpu = parse_quantity(allocated['cpu'])
+            total_cpu = parse_quantity(allocatable["cpu"])
+            allocated_cpu = parse_quantity(allocated["cpu"])
 
-            total_memory = parse_quantity(allocatable['memory'])
-            allocated_memory = parse_quantity(allocated['memory'])
+            total_memory = parse_quantity(allocatable["memory"])
+            allocated_memory = parse_quantity(allocated["memory"])
 
-            status = next((cond['status'] for cond in cluster['status']['conditions'] if cond['reason'] == 'ClusterReady'), None)
-            availability = True if status == 'True' else False
+            status = next(
+                (
+                    cond["status"]
+                    for cond in cluster["status"]["conditions"]
+                    if cond["reason"] == "ClusterReady"
+                ),
+                None,
+            )
+            availability = True if status == "True" else False
 
             result[cluster_name] = {
-                'total_cpu': float(total_cpu),
-                'allocated_cpu': float(allocated_cpu),
-                'remaining_cpu': float(total_cpu - allocated_cpu),
-                'total_memory_bytes': format_memory(total_memory),
-                'allocated_memory_bytes': format_memory(allocated_memory),
-                'remaining_memory_bytes': format_memory(total_memory - allocated_memory),
-                'availability': availability
+                "total_cpu": float(total_cpu),
+                "allocated_cpu": float(allocated_cpu),
+                "remaining_cpu": float(total_cpu - allocated_cpu),
+                "total_memory_bytes": format_memory(total_memory),
+                "allocated_memory_bytes": format_memory(allocated_memory),
+                "remaining_memory_bytes": format_memory(
+                    total_memory - allocated_memory
+                ),
+                "availability": availability,
             }
 
         return result
 
-
     def get_desired_replicas(self, name):
         """Returns the desired number of replicas for the specified deployment."""
 
-        response = self.v1_api_client.read_namespaced_deployment_scale(name, self.namespace)
+        response = self.v1_api_client.read_namespaced_deployment_scale(
+            name, self.namespace
+        )
         return response.spec.replicas
 
-
     def get_replicas(self, name):
         """Returns the current number of replicas for the specified deployment."""
 
@@ -69,18 +95,16 @@ class KarmadaHelper():
 
         return response.status.available_replicas
 
-
     def get_cpu_limit(self, name):
         """Returns the current CPU limit for the specific deployment."""
 
         response = self.v1_api_client.read_namespaced_deployment(name, self.namespace)
-        cpu_lim = response.spec.template.spec.containers[0].resources.limits['cpu']
-        if 'm' in cpu_lim:
-            return float(cpu_lim.replace('m', '')) * 1e-3
+        cpu_lim = response.spec.template.spec.containers[0].resources.limits["cpu"]
+        if "m" in cpu_lim:
+            return float(cpu_lim.replace("m", "")) * 1e-3
         else:
             return float(cpu_lim)
 
-
     def scale_deployment(self, name, replicas):
         """Scales the given application to the desired number of replicas"""
 
@@ -88,7 +112,7 @@ class KarmadaHelper():
             self.v1_api_client.patch_namespaced_deployment_scale(
                 name=name,
                 namespace=self.namespace,
-                body={'spec': {'replicas': replicas}}
+                body={"spec": {"replicas": replicas}},
             )
         except Exception as exception:
             print(str(exception))
diff --git a/src/utils/scaling.py b/src/utils/scaling.py
index ce07799..a08a545 100644
--- a/src/utils/scaling.py
+++ b/src/utils/scaling.py
@@ -10,40 +10,71 @@ from utils.prometheus_helper import PrometheusHelper
 
 
 def scaling_loop(
-    graph_name, acceleration, alpha, beta, cluster_capacity, cluster_acceleration,
-    maximum_replicas, managed_services, decision_interval, config_file_path,
-    prometheus_host, stop_event
+    graph_name,
+    acceleration,
+    alpha,
+    beta,
+    cluster_capacity,
+    cluster_acceleration,
+    maximum_replicas,
+    managed_services,
+    decision_interval,
+    config_file_path,
+    prometheus_host,
+    stop_event,
 ):
     """Runs the scaling algorithm periodically."""
 
     karmada_helper = KarmadaHelper(config_file_path)
     prometheus_helper = PrometheusHelper(prometheus_host, decision_interval)
     while True:
-        previous_replicas = [karmada_helper.get_replicas(service) for service in managed_services]
+        previous_replicas = [
+            karmada_helper.get_replicas(service) for service in managed_services
+        ]
         if None in previous_replicas:
             time.sleep(5)
         else:
             break
 
-    previous_replicas = [karmada_helper.get_replicas(service) for service in managed_services]
+    previous_replicas = [
+        karmada_helper.get_replicas(service) for service in managed_services
+    ]
     cpu_limits = [karmada_helper.get_cpu_limit(service) for service in managed_services]
 
     while not stop_event.is_set():
         request_rates = []
         for service in managed_services:
-            if service == 'image-compression-vo':
-                request_rates.append(prometheus_helper.get_request_rate('noise-reduction'))
+            if service == "image-compression-vo":
+                request_rates.append(
+                    prometheus_helper.get_request_rate("noise-reduction")
+                )
             else:
                 request_rates.append(prometheus_helper.get_request_rate(service))
-        print(request_rates, previous_replicas, cpu_limits, acceleration, alpha,
-            beta, cluster_capacity, cluster_acceleration, maximum_replicas)
+        print(
+            request_rates,
+            previous_replicas,
+            cpu_limits,
+            acceleration,
+            alpha,
+            beta,
+            cluster_capacity,
+            cluster_acceleration,
+            maximum_replicas,
+        )
 
         new_replicas = decide_replicas(
-            request_rates, previous_replicas, cpu_limits, acceleration, alpha,
-            beta, cluster_capacity, cluster_acceleration, maximum_replicas
+            request_rates,
+            previous_replicas,
+            cpu_limits,
+            acceleration,
+            alpha,
+            beta,
+            cluster_capacity,
+            cluster_acceleration,
+            maximum_replicas,
         )
         if new_replicas is None:
-            requests.get(f'http://localhost:8000/graphs/{graph_name}/placement')
+            requests.get(f"http://localhost:8000/graphs/{graph_name}/placement")
         else:
             for idx, replicas in enumerate(new_replicas):
                 karmada_helper.scale_deployment(managed_services[idx], replicas)
@@ -54,8 +85,15 @@ def scaling_loop(
 
 
 def decide_replicas(
-    request_rates, previous_replicas, cpu_limits, acceleration, alpha, beta,
-    cluster_capacity, cluster_acceleration, maximum_replicas
+    request_rates,
+    previous_replicas,
+    cpu_limits,
+    acceleration,
+    alpha,
+    beta,
+    cluster_capacity,
+    cluster_acceleration,
+    maximum_replicas,
 ):
     """
     Parameters
@@ -79,8 +117,12 @@ def decide_replicas(
     num_nodes = len(previous_replicas)
 
     # Decision variables
-    r_current = [cp.Variable(integer=True, name=f"r_current_{s}") for s in range(num_nodes)]
-    abs_diff = [cp.Variable(nonneg=True, name=f"abs_diff_{s}") for s in range(num_nodes)]
+    r_current = [
+        cp.Variable(integer=True, name=f"r_current_{s}") for s in range(num_nodes)
+    ]
+    abs_diff = [
+        cp.Variable(nonneg=True, name=f"abs_diff_{s}") for s in range(num_nodes)
+    ]
 
     w_util = 0.4
     w_trans = 0.4
@@ -98,7 +140,8 @@ def decide_replicas(
 
     # Cluster CPU capacity constraint
     constraints.append(
-        cp.sum([cpu_limits[s] * r_current[s] for s in range(num_nodes)]) <= cluster_capacity
+        cp.sum([cpu_limits[s] * r_current[s] for s in range(num_nodes)])
+        <= cluster_capacity
     )
 
     # Per-node constraints
@@ -108,8 +151,11 @@ def decide_replicas(
         constraints.append(r_current[s] >= 1)
 
     objective = cp.Minimize(
-        w_util * cp.sum([r_current[s] * cpu_limits[s] / max_util_cost for s in range(num_nodes)]) +
-        w_trans * cp.sum([abs_diff[s] / max_trans_cost[s] for s in range(num_nodes)])
+        w_util
+        * cp.sum(
+            [r_current[s] * cpu_limits[s] / max_util_cost for s in range(num_nodes)]
+        )
+        + w_trans * cp.sum([abs_diff[s] / max_trans_cost[s] for s in range(num_nodes)])
     )
 
     problem = cp.Problem(objective, constraints)
